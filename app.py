# Add this at the very top of your main application file (app.py)
import sys
import sqlite3

import os
import sys

# Debugging output
print(f"Python version: {sys.version}")
print(f"Current directory: {os.getcwd()}")
print(f"Files in data dir: {os.listdir('data')}")

# Workaround for pipeline import
try:
    from transformers import pipeline
except ImportError:
    from transformers.pipelines import pipeline
    print("Used fallback pipeline import")

# Check SQLite version and use pysqlite3 if needed
sqlite_version = sqlite3.sqlite_version_info
if sqlite_version < (3, 35, 0):
    try:
        # Replace sqlite3 with pysqlite3
        import pysqlite3 as sqlite3
        sys.modules['sqlite3'] = sqlite3
        print(f"Using pysqlite3-binary (SQLite {sqlite3.sqlite_version})")
    except ImportError:
        print("pysqlite3-binary not installed. Install with: pip install pysqlite3-binary")
        raise

# Now import ChromaDB and other dependencies
import chromadb


import os
import re
import unicodedata
import shutil
import torch
from transformers import pipeline
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import google.generativeai as genai
from tavily import TavilyClient
import streamlit as st

import sys

# Force use of modern SQLite via pysqlite3
try:
    import pysqlite3
    sys.modules["sqlite3"] = pysqlite3
except ImportError:
    pass


# Utility Functions
def initialize_gemini(model_name):
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        raise ValueError("GEMINI_API_KEY is not set.")
    genai.configure(api_key=gemini_api_key)
    generation_config = {
        "temperature": 0.7, "top_p": 0.9, "top_k": 40,
        "max_output_tokens": 4096, "response_mime_type": "text/plain",
    }
    return genai.GenerativeModel(model_name=model_name, generation_config=generation_config)

def remove_non_sinhala(text):
    """Remove characters not in the Sinhala Unicode range or basic Latin punctuation/digits."""
    allowed_ranges = [
        (0x0D80, 0x0DFF),  # Sinhala Unicode range
        (0x0020, 0x007E),  # Basic Latin (punctuation and digits)
    ]
    return ''.join(c for c in text if any(start <= ord(c) <= end for start, end in allowed_ranges))

def clean_response(response_text):
    """Clean the response text by removing unwanted patterns and non-Sinhala characters."""
    processed_response = response_text.replace("à¶¸à·–à¶½à·à·à·Šâ€à¶»: /content/context_facts.csv", "")
    processed_response = re.sub(r"\(à¶¸à·–à¶½à·à·à·Šâ€à¶» \d+(?:, \d+)*\)", "", processed_response)
    processed_response = unicodedata.normalize('NFC', processed_response).replace('\u200d', '')
    processed_response = remove_non_sinhala(processed_response)
    return processed_response

def setup_vector_store(csv_path, persist_directory, force_rebuild=False):
    os.makedirs(os.path.dirname(persist_directory), exist_ok=True)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
        model_kwargs={"device": device}
    )
    if os.path.exists(persist_directory) and os.path.isdir(persist_directory) and not force_rebuild:
        try:
            vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)
            return vector_store, vector_store._collection.count()
        except Exception as e:
            print(f"Error loading vector store: {e}")
            print("Will create a new vector store")
    if not csv_path:
        raise ValueError("CSV file path is required")
    if force_rebuild and os.path.exists(persist_directory):
        shutil.rmtree(persist_directory)
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV file not found at {csv_path}")
    loader = CSVLoader(file_path=csv_path)
    documents = loader.load()
    documents = [doc for doc in documents if doc.page_content.strip()]
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    vector_store = Chroma.from_documents(documents=texts, embedding=embedding_model, persist_directory=persist_directory)
    return vector_store, len(texts)

def retrieve_relevant_documents(question, vector_store, k=5):
    try:
        docs = vector_store.similarity_search(question, k=k)
        results = []
        for i, doc in enumerate(docs):
            content = doc.page_content if hasattr(doc, 'page_content') else ""
            if not content.strip():
                continue
            metadata = doc.metadata if hasattr(doc, 'metadata') else {}
            source = metadata.get('source', 'Unknown source')
            results.append({'content': content, 'source': source})
        return results
    except Exception as e:
        print(f"Error retrieving documents: {e}")
        return []

def initialize_search_clients():
    clients = {"available": False}
    tavily_api_key = os.getenv("TAVILY_API_KEY")
    if tavily_api_key:
        try:
            clients["tavily"] = TavilyClient(tavily_api_key)
            clients["available"] = True
        except Exception as e:
            print(f"Error initializing Tavily client: {e}")
    return clients

RECOMMENDED_SITES = [
    "https://hashtaggeneration.org/fact-check/",
    "https://srilanka.factcrescendo.com/",
    "https://www.bbc.com/sinhala",
    "https://sinhala.newsfirst.lk/",
    "https://www.adaderana.lk/",
    "https://www.hirunews.lk/english/",
    "https://www.cbsl.gov.lk/si"
]

# Agent Definitions
class DomainClassificationAgent:
    def __init__(self, gemini_model):
        self.gemini_model = gemini_model

    def classify(self, statement):
        chat_session = self.gemini_model.start_chat()
        prompt = f"""
        à¶´à·„à¶­ à·ƒà·’à¶‚à·„à¶½ à¶´à·Šâ€à¶»à¶šà·à·à¶º à¶šà·”à¶¸à¶± à·€à·’à·‚à¶º à¶šà·Šà·‚à·šà¶­à·Šâ€à¶»à¶ºà¶§ à¶…à¶ºà¶­à·Šà¶¯ à¶ºà¶±à·Šà¶± à¶­à·“à¶»à¶«à¶º à¶šà¶»à¶±à·Šà¶±: '{statement}'
        à·€à·’à·‚à¶º à¶šà·Šà·‚à·šà¶­à·Šâ€à¶»: politics, economics, health
        à¶”à¶¶à·š à¶­à·“à¶»à¶«à¶º à¶‘à¶šà·Š à·€à¶ à¶±à¶ºà¶šà·’à¶±à·Š à¶´à¶¸à¶«à¶šà·Š à¶½à¶¶à· à¶¯à·™à¶±à·Šà¶±à·Š (à¶‹à¶¯à·: politics).
        """
        response = chat_session.send_message(prompt)
        domain = response.text.strip().lower()
        if domain.startswith("à¶­à·“à¶»à¶«à¶º:"):
            domain = domain.replace("à¶­à·“à¶»à¶«à¶º:", "").strip()
        return domain

class DataRetrievalAgent:
    def __init__(self, domain, vector_store):
        self.domain = domain
        self.vector_store = vector_store

    def retrieve(self, statement):
        return retrieve_relevant_documents(statement, self.vector_store)

class DecisionAgent:
    def __init__(self, gemini_model):
        self.model = gemini_model

    def decide(self, statement, retrieved_docs):
        chat_session = self.model.start_chat()
        prompt = f"""
        à¶´à·„à¶­ à·ƒà·’à¶‚à·„à¶½ à¶´à·Šâ€à¶»à¶šà·à·à¶º à·ƒà¶³à·„à· à¶½à¶¶à· à¶¯à·“ à¶‡à¶­à·’ à¶­à·œà¶»à¶­à·”à¶»à·” à¶´à·Šâ€à¶»à¶¸à·à¶«à·€à¶­à·Šà¶¯ à¶ºà¶±à·Šà¶± à¶­à·“à¶»à¶«à¶º à¶šà¶»à¶±à·Šà¶±: '{statement}'

        à¶½à¶¶à· à¶‡à¶­à·’ à¶­à·œà¶»à¶­à·”à¶»à·”: {retrieved_docs}

        à¶­à·œà¶»à¶­à·”à¶»à·” à¶´à·Šâ€à¶»à¶¸à·à¶«à·€à¶­à·Š à¶ºà·à¶ºà·’ à·ƒà·à¶½à¶šà·™à¶±à·Šà¶±à·š, à¶‘à¶¸ à¶­à·œà¶»à¶­à·”à¶»à·” à¶¸à¶œà·’à¶±à·Š à¶´à·Šâ€à¶»à¶šà·à·à¶ºà·š à·ƒà¶³à·„à¶±à·Š à¶šà¶»à·”à¶«à·” à¶´à·’à·…à·’à¶¶à¶³ à¶±à·’à·à·Šà¶ à·’à¶­ à¶­à·œà¶»à¶­à·”à¶»à·” à¶­à·’à¶¶à·š à¶±à¶¸à·Š à¶´à¶¸à¶«à·’.
        à¶¸à·™à¶¸ à¶­à·œà¶»à¶­à·”à¶»à·”  à¶½à¶¶à·à¶¯à·“ à¶‡à¶­à·’ à¶´à·Šâ€à¶»à¶šà·à·à¶ºà·š à·ƒà¶­à·Šâ€à¶º à¶…à·ƒà¶­à·Šâ€à¶º à¶¶à·€ à¶±à·’à¶œà¶¸à¶±à¶º à·ƒà¶¯à·„à· à¶ºà·œà¶¯à·à¶œà¶±à·Šà¶±à· à¶¶à·à·€à·’à¶±à·Š à·€à¶œà¶šà·“à¶¸à·Š à·ƒà·„à¶œà¶­ à¶½à·™à·ƒ à¶´à·’à·…à·’à¶­à·”à¶»à·” à¶½à¶¶à·à¶¯à·™à¶±à·Šà¶±.

        à¶šà¶»à·”à¶«à·à¶šà¶», à¶”à¶¶à·š à¶´à·’à·…à·’à¶­à·”à¶»à·š à¶´à·…à¶¸à·” à¶´à·šà·…à·’à¶º à·ƒà·Šà¶´à·ƒà·Šà¶§ "à¶±à·’à¶œà¶¸à¶±à¶º: [à·ƒà¶­à·Šâ€à¶º/à¶…à·ƒà¶­à·Šâ€à¶º/à¶­à·“à¶»à¶«à¶º à¶šà·… à¶±à·œà·„à·à¶š]" à¶½à·™à·ƒ à¶½à¶¶à· à¶¯à·™à¶±à·Šà¶±.
        """
        response = chat_session.send_message(prompt)
        response_text = response.text.strip().lower()
        # Search for the first line starting with "à¶±à·’à¶œà¶¸à¶±à¶º:" and use that for verdict extraction.
        for line in response_text.split('\n'):
            line = line.strip()
            if line.startswith("à¶±à·’à¶œà¶¸à¶±à¶º:"):
                verdict_line = line
                break
        else:
            verdict_line = ""
        if re.search(r"à¶±à·’à¶œà¶¸à¶±à¶º:\s*à·ƒà¶­à·Šâ€?à¶º", verdict_line):
            return "sufficient"
        else:
            return "insufficient"

class FactAnalysisAgent:
    def __init__(self, gemini_model):
        self.model = gemini_model

    def verify_with_rag(self, claim, vector_store):
        retrieved_docs = retrieve_relevant_documents(claim, vector_store)
        if not retrieved_docs:
            return ("à¶¸à¶§ à¶šà¶±à¶œà·à¶§à·”à¶ºà·’, à¶”à¶¶à·š à¶´à·Šâ€à¶»à¶šà·à·à¶º à¶´à¶»à·“à¶šà·Šà·‚à· à¶šà·’à¶»à·“à¶¸à¶§ à¶…à·€à·à·Šâ€à¶º à¶­à·œà¶»à¶­à·”à¶»à·” à·ƒà·œà¶ºà·à¶œà¶­ à¶±à·œà·„à·à¶šà·’ à·€à·’à¶º. "
                    "à¶šà¶»à·”à¶«à·à¶šà¶» à·€à·™à¶±à¶­à·Š à¶´à·Šâ€à¶»à¶šà·à·à¶ºà¶šà·Š à¶‹à¶­à·Šà·ƒà·à·„ à¶šà¶»à¶±à·Šà¶±.")
        chat_session = self.model.start_chat()
        final_prompt = f"""
        à¶”à¶¶ à·ƒà·’à¶‚à·„à¶½ à¶´à·Šâ€à¶»à¶šà· à·€à¶½ à·ƒà¶­à·Šâ€à¶º à¶…à·ƒà¶­à·Šâ€à¶º à¶¶à·€ à·ƒà·à¶šà·Šà·‚à·’ à·ƒà¶¸à¶œ à·ƒà·„à¶­à·’à¶š à¶šà¶»à¶± AI à·ƒà¶·à·à¶ºà¶šà¶ºà·™à¶šà·’.
        à¶šà¶»à·”à¶«à·à¶šà¶» à¶”à¶¶à·š à¶±à·’à¶œà¶¸à¶±à¶º à¶´à·…à¶¸à·” à¶´à·šà·…à·’à¶º "à¶±à·’à¶œà¶¸à¶±à¶º: [à·ƒà¶­à·Šâ€à¶º/à¶…à·ƒà¶­à·Šâ€à¶º/à¶­à·“à¶»à¶«à¶º à¶šà·… à¶±à·œà·„à·à¶š]" à¶½à·™à·ƒ à¶½à¶¶à· à¶¯à·™à¶±à·Šà¶±.
        à¶´à·„à¶­ à¶´à·Šâ€à¶»à¶šà·à·à¶º à·€à·’à¶¸à¶»à·Šà·à¶±à¶º à¶šà¶»à¶±à·Šà¶±: '{claim}'
        à¶½à¶¶à· à¶‡à¶­à·’ à¶­à·œà¶»à¶­à·”à¶»à·”: {retrieved_docs}
        à¶šà¶»à·”à¶«à·à¶šà¶»:
        1. à¶´à·…à¶¸à·” à¶´à·šà·…à·’à¶ºà¶§ à¶´à¶¸à¶«à¶šà·Š à¶”à¶¶à·š à¶±à·’à¶œà¶¸à¶±à¶º (à¶‹à¶¯à·: à¶±à·’à¶œà¶¸à¶±à¶º: à·ƒà¶­à·Šâ€à¶º)
        2. à¶‰à¶­à·’à¶»à·’ à¶´à·šà·…à·’ à·€à¶½ à¶”à¶¶à·š à·€à·’à·ƒà·Šà¶­à¶»à·à¶­à·Šà¶¸à¶š à·„à·šà¶­à·” à·ƒà¶³à·„à¶±à·Š à¶šà¶»à¶±à·Šà¶±.
        """
        response = chat_session.send_message(final_prompt)
        return clean_response(response.text)

    def verify_with_search(self, claim, search_clients):
        if not search_clients.get("available"):
            return ("à¶¸à¶§ à¶šà¶±à¶œà·à¶§à·”à¶ºà·’, à·ƒà·™à·€à·”à¶¸à·Š à¶ºà·à¶±à·Šà¶­à·Šâ€à¶»à¶«à¶º à¶·à·à·€à·’à¶­à· à¶šà·… à¶±à·œà·„à·à¶šà·’ à¶¶à·à·€à·’à¶±à·Š "
                    "à¶­à·„à·€à·”à¶»à·” à¶šà·’à¶»à·“à¶¸à·Š à·ƒà·’à¶¯à·” à¶šà·… à¶±à·œà·„à·à¶š. API à¶ºà¶­à·”à¶»à·” à·ƒà·à¶šà·ƒà·“à¶¸à·™à¶±à·Š à¶´à·ƒà·” à¶±à·à·€à¶­ à¶‹à¶­à·Šà·ƒà·à·„ à¶šà¶»à¶±à·Šà¶±.")
        tavily_client = search_clients.get("tavily")
        if not tavily_client:
            return ("à¶¸à¶§ à¶šà¶±à¶œà·à¶§à·”à¶ºà·’, Tavily à·ƒà·™à·€à·”à¶¸à·Š à¶ºà·à¶±à·Šà¶­à·Šâ€à¶»à¶«à¶º à¶·à·à·€à·’à¶­à· à¶šà·… à¶±à·œà·„à·à¶šà·’ à¶¶à·à·€à·’à¶±à·Š "
                    "à¶­à·„à·€à·”à¶»à·” à¶šà·’à¶»à·“à¶¸à·Š à·ƒà·’à¶¯à·” à¶šà·… à¶±à·œà·„à·à¶š.")
        try:
            tavily_results = tavily_client.search(
                query=claim + " Sri Lanka", search_depth="advanced", include_domains=RECOMMENDED_SITES
            )
            fast_check_results = [
                {'title': result['title'], 'content': result['content']}
                for result in tavily_results.get('results', []) if 'title' in result and 'content' in result
            ]
            if not fast_check_results:
                return ("à¶¸à¶§ à¶šà¶±à¶œà·à¶§à·”à¶ºà·’, à¶”à¶¶à·š à¶´à·Šâ€à¶»à¶šà·à·à¶º à·ƒà¶¸à·Šà¶¶à¶±à·Šà¶° à¶­à·œà¶»à¶­à·”à¶»à·” à¶…à¶±à·Šà¶­à¶»à·Šà¶¢à·à¶½à¶ºà·™à¶±à·Š à·ƒà·œà¶ºà·à¶œà¶­ à¶±à·œà·„à·à¶šà·’ à·€à·’à¶º.")
            chat_session = self.model.start_chat()
            final_prompt = f"""
            à¶”à¶¶ à·ƒà·’à¶‚à·„à¶½ à¶´à·Šâ€à¶»à¶šà· à·€à¶½ à·ƒà¶­à·Šâ€à¶º à¶…à·ƒà¶­à·Šâ€à¶º à¶¶à·€ à·ƒà·à¶šà·Šà·‚à·’ à·ƒà¶¸à¶œ à·ƒà·„à¶­à·’à¶š à¶šà¶»à¶± AI à·ƒà¶·à·à¶ºà¶šà¶ºà·™à¶šà·’.
            à¶šà¶»à·”à¶«à·à¶šà¶» à¶”à¶¶à·š à¶±à·’à¶œà¶¸à¶±à¶º à¶´à·…à¶¸à·” à¶´à·šà·…à·’à¶º "à¶±à·’à¶œà¶¸à¶±à¶º: [à·ƒà¶­à·Šâ€à¶º/à¶…à·ƒà¶­à·Šâ€à¶º/à¶­à·“à¶»à¶«à¶º à¶šà·… à¶±à·œà·„à·à¶š]" à¶½à·™à·ƒ à¶½à¶¶à· à¶¯à·™à¶±à·Šà¶±.
            à¶´à·„à¶­ à¶´à·Šâ€à¶»à¶šà·à·à¶º à·€à·’à¶¸à¶»à·Šà·à¶±à¶º à¶šà¶»à¶±à·Šà¶±à·Š: '{claim}'
            à¶¸à·™à¶¸ à¶´à·Šâ€à¶»à¶šà·à·à¶º à·ƒà¶­à·Šâ€à¶ºà·à¶´à¶±à¶º à¶šà·’à¶»à·“à¶¸ à·ƒà¶³à·„à· à¶´à·„à¶­ à·ƒà·™à·€à·”à¶¸à·Š à¶´à·Šâ€à¶»à¶­à·’à¶µà¶½ à·ƒà¶½à¶šà· à¶¶à¶½à¶±à·Šà¶±:
            {fast_check_results}
            """
            response = chat_session.send_message(final_prompt)
            return clean_response(response.text)
        except Exception as e:
            return f"à¶¸à¶§ à¶šà¶±à¶œà·à¶§à·”à¶ºà·’, à·ƒà·™à·€à·“à¶¸à·šà¶¯à·“ à¶¯à·à·‚à¶ºà¶šà·Š à¶‡à¶­à·’à·€à·’à¶º: {str(e)}"

    def verify_combined(self, claim, vector_store, search_clients):
        rag_result = self.verify_with_rag(claim, vector_store)
        search_result = self.verify_with_search(claim, search_clients)
        chat_session = self.model.start_chat()
        final_prompt = f"""
        à¶”à¶¶ à·ƒà·’à¶‚à·„à¶½ à¶´à·Šâ€à¶»à¶šà· à·€à¶½ à·ƒà¶­à·Šâ€à¶º à¶…à·ƒà¶­à·Šâ€à¶º à¶¶à·€ à·ƒà·à¶šà·Šà·‚à·’ à·ƒà¶¸à¶œ à·ƒà·„à¶­à·’à¶š à¶šà¶»à¶± AI à·ƒà¶·à·à¶ºà¶šà¶ºà·™à¶šà·’.
        à¶šà¶»à·”à¶«à·à¶šà¶» à¶”à¶¶à·š à¶±à·’à¶œà¶¸à¶±à¶º à¶´à·…à¶¸à·” à¶´à·šà·…à·’à¶º "à¶±à·’à¶œà¶¸à¶±à¶º: [à·ƒà¶­à·Šâ€à¶º/à¶…à·ƒà¶­à·Šâ€à¶º/à¶­à·“à¶»à¶«à¶º à¶šà·… à¶±à·œà·„à·à¶š]" à¶½à·™à·ƒ à¶½à¶¶à· à¶¯à·™à¶±à·Šà¶±.
        à¶´à·„à¶­ à¶´à·Šâ€à¶»à¶šà·à·à¶º à·€à·’à¶¸à¶»à·Šà·à¶±à¶º à¶šà¶»à¶±à·Šà¶±: '{claim}'
        à¶¸à·™à¶¸ à¶´à·Šâ€à¶»à¶šà·à·à¶º à·ƒà¶­à·Šâ€à¶ºà·à¶´à¶±à¶º à¶šà·’à¶»à·“à¶¸ à·ƒà¶³à·„à· à¶´à·„à¶­ à¶´à·Šâ€à¶»à¶­à·’à¶µà¶½ à¶½à·à¶¶à·“ à¶‡à¶­:
        1. à·€à·™à¶šà·Šà¶§à¶»à·Š à¶¯à¶­à·Šà¶­ à¶œà¶¶à¶©à·à·€à·™à¶±à·Š (RAG) à¶½à·à¶¶à·”à¶«à·” à¶´à·Šâ€à¶»à¶­à·’à¶µà¶½à¶º: {rag_result}
        2. à·ƒà¶¢à·“à·€à·“ à¶…à¶±à·Šà¶­à¶»à·Šà¶¢à·à¶½ à·ƒà·™à·€à·“à¶¸à·™à¶±à·Š à¶½à·à¶¶à·”à¶«à·” à¶´à·Šâ€à¶»à¶­à·’à¶µà¶½à¶º: {search_result}
        à¶‰à·„à¶­ à¶¯à·™à¶šà¶¸ à·ƒà¶½à¶šà· à¶¶à·à¶½à·“à¶¸à·™à¶±à·Š, à¶šà¶»à·”à¶«à·à¶šà¶» à¶´à·…à¶¸à·” à¶´à·šà·…à·’à¶ºà·š à¶´à¶¸à¶«à¶šà·Š à¶”à¶¶à·š à¶±à·’à¶œà¶¸à¶±à¶º (à¶‹à¶¯à·: à¶±à·’à¶œà¶¸à¶±à¶º: à·ƒà¶­à·Šâ€à¶º) à¶½à¶¶à· à¶¯à·™à¶±à·Šà¶±,
        à·„à· à¶‰à¶­à·’à¶»à·’ à¶´à·šà·…à·’ à·€à¶½ à·„à·šà¶­à·” à·€à·’à·ƒà·Šà¶­à¶» à¶šà¶»à¶±à·Šà¶±.
        """
        response = chat_session.send_message(final_prompt)
        return clean_response(response.text)

class VerdictAgent:
    def extract_verdict(self, analysis):
        """
        Search through the analysis for the first line starting with "à¶±à·’à¶œà¶¸à¶±à¶º:" and extract the verdict.
        """
        analysis_lines = analysis.split('\n')
        verdict_line = ""
        for line in analysis_lines:
            line = line.strip()
            if line.startswith("à¶±à·’à¶œà¶¸à¶±à¶º:"):
                verdict_line = line
                break

        # Normalize the text to handle Unicode variations
        normalized_line = unicodedata.normalize('NFC', verdict_line)
        if re.search(r"à¶±à·’à¶œà¶¸à¶±à¶º:\s*à·ƒà¶­à·Šâ€?à¶º", normalized_line):
            return "true"
        elif re.search(r"à¶±à·’à¶œà¶¸à¶±à¶º:\s*à¶…à·ƒà¶­à·Šâ€?à¶º", normalized_line):
            return "false"
        else:
            return "insufficient information"

class OrchestrationAgent:
    def __init__(self, domain_classifier, retrieval_agents, fact_analyzer, verdict_agent, search_clients, decision_agent):
        self.domain_classifier = domain_classifier
        self.retrieval_agents = retrieval_agents
        self.fact_analyzer = fact_analyzer
        self.verdict_agent = verdict_agent
        self.search_clients = search_clients
        self.decision_agent = decision_agent

    def verify_statement(self, statement, method="rag"):
        if method not in ["rag", "search", "both", "automatic"]:
            raise ValueError("Invalid method. Choose from 'rag', 'search', 'both', 'automatic'.")

        domain = self.domain_classifier.classify(statement)
        if domain not in self.retrieval_agents:
            raise ValueError(f"Domain '{domain}' not supported. Supported domains: {list(self.retrieval_agents.keys())}")

        if method == "automatic":
            retrieval_agent = self.retrieval_agents[domain]
            vector_store = retrieval_agent.vector_store
            retrieved_docs = retrieve_relevant_documents(statement, vector_store)
            decision = self.decision_agent.decide(statement, retrieved_docs)
            if decision == "sufficient":
                analysis = self.fact_analyzer.verify_with_rag(statement, vector_store)
                method_used = "à·ƒà·Šà¶®à·à¶±à·“à¶º à¶¯à¶­à·Šà¶­ (RAG)"
            else:
                analysis = self.fact_analyzer.verify_with_search(statement, self.search_clients)
                method_used = "à¶…à¶±à·Šà¶­à¶»à·Šà¶¢à·à¶½ à·ƒà·™à·€à·“à¶¸"
        else:
            retrieval_agent = self.retrieval_agents[domain]
            vector_store = retrieval_agent.vector_store
            if method == "rag":
                analysis = self.fact_analyzer.verify_with_rag(statement, vector_store)
                method_used = "à·ƒà·Šà¶®à·à¶±à·“à¶º à¶¯à¶­à·Šà¶­"
            elif method == "search":
                analysis = self.fact_analyzer.verify_with_search(statement, self.search_clients)
                method_used = "à¶…à¶±à·Šà¶­à¶»à·Šà¶¢à·à¶½ à·ƒà·™à·€à·“à¶¸"
            elif method == "both":
                analysis = self.fact_analyzer.verify_combined(statement, vector_store, self.search_clients)
                method_used = "à·ƒà·Šà¶®à·à¶±à·“à¶º à¶¯à¶­à·Šà¶­ à·ƒà·„ à¶…à¶±à·Šà¶­à¶»à·Šà¶¢à·à¶½ à·ƒà·™à·€à·“à¶¸"

        verdict = self.verdict_agent.extract_verdict(analysis)
        return verdict, analysis, method_used

@st.cache_resource
def initialize_app():
    try:
        # Get current script directory for absolute paths
        current_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(current_dir, 'data')
        
        st.write("ðŸ” Debug Info:")
        st.write(f"Current script directory: {current_dir}")
        st.write(f"Data directory: {data_dir}")
        
        # Verify data directory exists
        if not os.path.exists(data_dir):
            st.error(f"âŒ 'data' directory not found at: {data_dir}")
            return None
            
        # List files in data directory
        files_in_data = os.listdir(data_dir)
        st.write(f"Files in data directory: {files_in_data}")
        
        # Find CSV files with absolute paths
        csv_files_found = [f for f in files_in_data if f.endswith('.csv')]
        st.write(f"CSV files found: {csv_files_found}")
        
        # Build csv_paths with absolute paths
        csv_paths = {}
        file_domain_mapping = {
            'economics_data.csv': 'economics',
            'politics_data.csv': 'politics', 
            'health_data.csv': 'health'
        }
        
        for filename, domain in file_domain_mapping.items():
            full_path = os.path.join(data_dir, filename)
            if os.path.exists(full_path):
                csv_paths[domain] = full_path
                st.success(f"âœ… Found {domain} data: {full_path}")
            else:
                st.warning(f"âš ï¸ Missing {domain} data: {filename}")
        
        # Verify we found at least one CSV
        if not csv_paths:
            st.error("ðŸš« No CSV files found! Please check your data directory")
            return None
            
        st.info(f"ðŸ“ Using {len(csv_paths)} CSV file(s): {list(csv_paths.keys())}")
        
        # Load vector stores
        vector_stores = {}
        for domain, csv_path in csv_paths.items():
            try:
                persist_directory = f"chroma_db_{domain}"
                vector_store, doc_count = setup_vector_store(csv_path, persist_directory)
                vector_stores[domain] = vector_store
                st.success(f"âœ… Loaded {doc_count} documents for {domain}")
            except Exception as e:
                st.error(f"âŒ Error loading {domain} data: {str(e)}")
                st.exception(e)  # Show full traceback
        
        if not vector_stores:
            st.error("ðŸš« Failed to load any vector stores. Check your CSV files and try again.")
            return None
            
        return vector_stores
        
    except Exception as e:
        st.error(f"ðŸ”¥ Critical error in app initialization: {str(e)}")
        st.exception(e)
        return None
        
        # Initialize models
        general_model = initialize_gemini("models/gemma-3-27b-it")
        decision_model = initialize_gemini("models/gemma-3-27b-it")
        
        # Initialize agents
        retrieval_agents = {domain: DataRetrievalAgent(domain, vector_stores[domain]) 
                          for domain in vector_stores.keys()}
        domain_classifier = DomainClassificationAgent(general_model)
        fact_analyzer = FactAnalysisAgent(general_model)
        verdict_agent = VerdictAgent()
        search_clients = initialize_search_clients()
        decision_agent = DecisionAgent(decision_model)
        
        orchestrator = OrchestrationAgent(
            domain_classifier, retrieval_agents, fact_analyzer, 
            verdict_agent, search_clients, decision_agent
        )
        
        st.success(f"ðŸŽ‰ Application initialized successfully with {len(vector_stores)} domain(s)!")
        return orchestrator
        
    except Exception as e:
        st.error(f"ðŸ’¥ Error initializing application: {str(e)}")
        st.write("Full error details:", str(e))
        return None

# Main Streamlit UI
def main():
    st.set_page_config(page_title="à·ƒà·’à¶‚à·„à¶½ à·ƒà¶­à·Šâ€à¶º à·ƒà·™à·€à·”à¶¸à·Šà¶šà¶»à·”", layout="centered")
    st.title("à·ƒà·’à¶‚à·„à¶½ à·ƒà¶­à·Šâ€à¶º à·ƒà·™à·€à·”à¶¸à·Šà¶šà¶»à·”")
    st.markdown("**Sinhala Fact Verification System**")
    st.write("à¶¸à·™à¶¸ à¶´à¶¯à·Šà¶°à¶­à·’à¶º à¶”à¶¶ à¶½à¶¶à· à¶¯à·™à¶± à·ƒà·’à¶‚à·„à¶½ à¶´à·Šâ€à¶»à¶šà·à·à¶ºà¶±à·Šà·„à·’ à·ƒà¶­à·Šâ€à¶ºà¶­à·à·€à¶º à¶´à¶»à·“à¶šà·Šà·‚à· à¶šà¶»à¶ºà·’.")
    
    # Initialize the app
    orchestrator = initialize_app()
    if orchestrator is None:
        st.stop()
    
    with st.form("fact_check_form"):
        statement = st.text_area(
            "à·ƒà¶­à·Šâ€à¶ºà·à¶´à¶±à¶º à¶šà·’à¶»à·“à¶¸à¶§ à¶…à·€à·à·Šâ€à¶º à¶´à·Šâ€à¶»à¶šà·à·à¶º à¶‡à¶­à·”à·…à¶­à·Š à¶šà¶»à¶±à·Šà¶±",
            height=150,
            placeholder="à¶‹à¶¯à·à·„à¶»à¶«à¶º: à·à·Šâ€à¶»à·“ à¶½à¶‚à¶šà·à·€à·š à¶†à¶»à·Šà¶®à·’à¶šà¶º 2023 à¶¯à·“ 5% à¶šà·’à¶±à·Š à·€à¶»à·Šà¶°à¶±à¶º à·€à·’à¶º."
        )
        
        options = ["Automatic", "RAG only"]
        search_clients = initialize_search_clients()
        if search_clients.get("available", False):
            options.extend(["Search only", "Both"])
        
        method = st.selectbox("à¶­à·“à¶»à¶« à¶œà·à¶±à·“à¶¸à·š à¶šà·Šâ€à¶»à¶¸à¶º", options)
        submit_button = st.form_submit_button("à·ƒà¶­à·Šâ€à¶ºà·à¶´à¶±à¶º à¶šà¶»à¶±à·Šà¶±")

    if submit_button and statement:
        method_map = {
            "Automatic": "automatic",
            "RAG only": "rag",
            "Search only": "search",
            "Both": "both"
        }
        selected_method = method_map[method]
        
        with st.spinner("à¶´à·Šâ€à¶»à¶šà·à·à¶º à·ƒà¶­à·Šâ€à¶ºà·à¶´à¶±à¶º à¶šà¶»à¶¸à·’à¶±à·Š à¶´à·€à¶­à·“..."):
            try:
                verdict, analysis, method_used = orchestrator.verify_statement(statement, method=selected_method)
                
                st.subheader("à¶±à·’à¶œà¶¸à¶±à¶º")
                st.markdown(f"**à¶·à·à·€à·’à¶­ à¶šà·… à¶šà·Šâ€à¶»à¶¸à¶º**: {method_used}")
                
                if verdict == "true":
                    st.success("**à·ƒà¶­à·Šâ€à¶ºà¶ºà·’**")
                elif verdict == "false":
                    st.error("**à¶…à·ƒà¶­à·Šâ€à¶ºà¶ºà·’**")
                else:
                    st.warning("**à¶­à·œà¶»à¶­à·”à¶»à·” à¶´à·Šâ€à¶»à¶¸à·à¶«à·€à¶­à·Š à¶±à·œà·€à·š**")

                # Remove verdict phrases from analysis
                cleaned_analysis = re.sub(r"à¶±à·’à¶œà¶¸à¶±à¶º:\s*(à·ƒà¶­à·Šà¶º|à¶…à·ƒà¶­à·Šà¶º|à¶­à·œà¶»à¶­à·”à¶»à·” à¶´à·Šâ€à¶»à¶¸à·à¶«à·€à¶­à·Š à¶±à·œà·€à·š)", "", analysis)
                st.subheader("à·€à·’à·à·Šà¶½à·šà·‚à¶«à¶º")
                st.write(cleaned_analysis)
            except Exception as e:
                st.error(f"Error during fact checking: {str(e)}")


if __name__ == "__main__":
    main()
# Run the Streamlit app
# To run the app, use the command: streamlit run app.py